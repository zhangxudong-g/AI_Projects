# 模型部署和使用经验总结文档

## 1. 概述

本文档总结了AI模型部署和使用的经验和最佳实践。涵盖了Ollama和vLLM两种主流模型部署方案，包括环境配置、性能优化、离线部署、API调用等方面的内容。

## 2. 模型部署方法和技术

### 2.1 Ollama模型部署

#### 2.1.1 环境配置
- **环境变量设置**：
  - `OLLAMA_HOST`: 服务器IP地址（默认127.0.0.1:11434）
  - `OLLAMA_MAX_LOADED_MODELS`: 每个GPU上最大加载模型数
  - `OLLAMA_NUM_PARALLEL`: 最大并行请求数
  - `OLLAMA_MAX_QUEUE`: 最大排队请求数
  - `OLLAMA_MODELS`: 模型存储路径
  - `OLLAMA_KEEP_ALIVE`: 模型在内存中的保留时间

- **系统服务配置**：
  - 使用systemd管理Ollama服务
  - 在`/etc/systemd/system/ollama.service`中配置服务参数
  - 设置适当的重启策略和环境变量

#### 2.1.2 性能参数调优
- **资源分配**：
  - `OLLAMA_MAX_LOADED_MODELS`: 每64GB内存允许加载1个模型
  - `OLLAMA_NUM_PARALLEL`: 每4个CPU核心允许1个并行任务
  - `OLLAMA_MAX_VRAM`: 设置GPU显存限制

- **服务配置示例**：
  ```ini
  [Unit]
  Description=Ollama Service
  After=network-online.target

  [Service]
  ExecStart=/usr/local/bin/ollama serve
  User=ollama
  Group=ollama
  Restart=always
  RestartSec=3
  Environment="OLLAMA_HOST=0.0.0.0:11434"
  Environment="OLLAMA_MAX_LOADED_MODELS=2"
  Environment="OLLAMA_NUM_PARALLEL=6"
  Environment="OLLAMA_MAX_QUEUE=1024"
  Environment="OLLAMA_MAX_VRAM=60GB"

  [Install]
  WantedBy=default.target
  ```

### 2.2 vLLM模型部署

#### 2.2.1 模型准备
- 从Hugging Face或其他模型库下载模型
- 验证模型文件完整性（config.json, tokenizer.json等）
- 配置模型路径和名称

#### 2.2.2 启动配置
- **核心参数**：
  - `--tensor-parallel-size`: 张量并行大小
  - `--max-model-len`: 最大模型长度
  - `--gpu-memory-utilization`: GPU内存利用率
  - `--dtype`: 数据类型（bfloat16推荐）

- **性能优化配置**：
  ```bash
  python -m vllm.entrypoints.openai.api_server \
    --model "${MODEL_DIR}" \
    --served-model-name "${SERVED_NAME}" \
    --tensor-parallel-size 2 \
    --max-model-len 8192 \
    --max-num-seqs 8 \
    --gpu-memory-utilization 0.80 \
    --dtype bfloat16 \
    --disable-log-requests \
    --trust-remote-code
  ```

#### 2.2.3 内存管理
- 配置CUDA内存管理：
  ```bash
  export CUDA_VISIBLE_DEVICES=0,1
  export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:256
  ```

### 2.3 离线模型部署

#### 2.3.1 模型打包
- **识别模型文件结构**：
  - manifests目录：存储模型清单文件
  - blobs目录：存储模型权重文件

- **打包模型**：
  ```bash
  sudo tar -czvf model-name.tar.gz \
    models/manifests/registry.ollama.ai/library/model/name \
    models/blobs/sha256-*
  ```

#### 2.3.2 模型传输和部署
- **传输方法**：
  - 使用`rsync`或`scp`传输大文件
  - 使用`docker save/load`传输模型镜像

- **部署步骤**：
  1. 解压模型文件到正确位置
  2. 修复文件权限：`sudo chown -R ollama:ollama /path/to/models`
  3. 验证模型完整性
  4. 重启服务

### 2.4 GGUF格式模型处理

#### 2.4.1 模型下载和合并
- 从ModelScope等平台下载GGUF格式模型
- 处理分片模型文件并合并：
  ```bash
  cat part1.gguf part2.gguf > complete_model.gguf
  ```

#### 2.4.2 模型创建
- 使用Modelfile定义模型配置：
  ```dockerfile
  FROM /path/to/model.gguf
  PARAMETER num_ctx 4096
  PARAMETER embedding true
  TEMPLATE """{{ .Prompt }}"""
  ```

- 创建模型：`ollama create model-name -f Modelfile`

#### 2.4.3 模型量化
- 使用gguf-tools进行模型量化：
  ```bash
  gguf-quant \
    --input model.gguf \
    --output quantized_model.gguf \
    --bits 8 \
    --perchannel
  ```

## 3. 模型使用方式和最佳实践

### 3.1 API调用方式

#### 3.1.1 REST API
- 使用标准HTTP请求调用模型
- 支持同步和异步调用
- 配置请求参数（temperature、max_tokens等）

#### 3.1.2 SDK调用
- 使用官方或社区提供的SDK
- 简化API调用流程
- 提供更高级的功能封装

#### 3.1.3 命令行工具
- 使用Ollama CLI工具
- 直接与模型交互
- 适合调试和测试

### 3.2 性能优化

#### 3.2.1 并发处理
- 合理设置并行请求数量
- 使用连接池管理请求
- 实现请求队列和限流

#### 3.2.2 内存管理
- 控制模型加载数量
- 设置合适的keep-alive时间
- 监控内存使用情况

#### 3.2.3 批处理优化
- 合并多个小请求
- 优化批处理大小
- 平衡延迟和吞吐量

### 3.3 模型选择和配置

#### 3.3.1 模型评估
- 根据任务需求选择合适模型
- 考虑参数量、推理速度、准确性
- 评估硬件资源要求

#### 3.3.2 参数调优
- 调整temperature、top_p等生成参数
- 设置合适的上下文长度
- 优化prompt设计

### 3.4 监控和日志

#### 3.4.1 性能监控
- 监控推理延迟
- 跟踪吞吐量指标
- 监控资源使用情况

#### 3.4.2 日志记录
- 记录请求和响应
- 跟踪错误和异常
- 收集性能数据

## 4. 部署策略和经验

### 4.1 方案选择
- **Ollama**：适合快速部署和简单使用场景，配置简单，易于上手
- **vLLM**：适合高性能、高并发生产环境，性能优异，但配置较复杂

### 4.2 资源规划
- 根据模型大小和并发需求规划硬件资源
- 考虑CPU、GPU、内存和存储需求
- 预留足够的缓冲空间应对峰值负载

### 4.3 性能调优经验
- 根据硬件规格调整并行处理参数
- 合理设置模型加载数量
- 优化内存和显存使用
- 使用多个实例分散请求负载

### 4.4 运维管理
- 实施全面的性能监控
- 设置关键指标告警
- 定期审查和优化配置
- 定期备份模型和配置
- 准备应急恢复方案

### 4.5 安全考虑
- 限制API访问权限
- 使用认证和授权机制
- 配置网络安全策略
- 加密敏感数据传输
- 管理模型和数据访问权限

## 5. 常见问题和解决方案

### 5.1 内存不足
- 调整`OLLAMA_MAX_LOADED_MODELS`参数
- 优化模型加载策略
- 增加物理内存或使用交换分区

### 5.2 性能瓶颈
- 检查硬件资源使用情况
- 调整并行处理参数
- 优化批处理大小

### 5.3 网络问题
- 检查防火墙配置
- 验证网络连接
- 使用镜像加速器

## 6. 总结

模型部署和使用是一个复杂的过程，需要综合考虑性能、可靠性、安全性和可维护性。选择合适的部署方案（Ollama或vLLM），合理配置参数，实施有效的监控和运维策略，是确保模型服务稳定运行的关键。随着AI技术的不断发展，模型部署和使用的方法也在不断演进，需要持续学习和实践以适应新的挑战。