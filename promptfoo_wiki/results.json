{
  "evalId": "eval-OXc-2026-01-30T05:47:08",
  "results": {
    "version": 3,
    "timestamp": "2026-01-30T05:47:08.454Z",
    "prompts": [
      {
        "id": "1f559caa9a1e0372ccf383637f46f55d94cdaf7ca4cc9a896a82b47754e15d44",
        "raw": "You are a senior software engineer reviewing generated documentation.\n\nYou are given:\n- SOURCE CODE (Java or SQL)\n- GENERATED WIKI (Markdown)\n\n--- SOURCE CODE ---\n{{source_code}}\n\n--- GENERATED WIKI ---\n{{wiki_md}}\n\nYour task:\nEvaluate whether the Wiki accurately and usefully documents the source code.\n\nThe source language MAY be:\n- Java (classes, methods, fields)\n- SQL (procedures, tables, columns, parameters)\n\nYou MUST judge based ONLY on the given content.\nYou MUST NOT ask for more files or clarification.\n\nEvaluation dimensions (0â€“25 each):\n\n1. Coverage (0â€“25)\n   - Are all major components documented?\n   - Java: classes, public methods, important fields\n   - SQL: procedures, parameters, tables, key logic\n\n2. Correctness (0â€“25)\n   - Are names, signatures, and behavior described correctly?\n   - No wrong return types, parameters, or purposes\n\n3. Hallucination (0â€“25)\n   - 25 = no invented behavior or objects\n   - Deduct heavily if the Wiki mentions things not present\n\n4. Usefulness (0â€“25)\n   - Is this Wiki useful for a developer maintaining the code?\n   - Clear structure, accurate summaries, meaningful explanations\n\nFinal Score = sum of all four dimensions (0â€“100)\n\nRespond ONLY in valid JSON.\n",
        "label": "You are a senior software engineer reviewing generated documentation.\n\nYou are given:\n- SOURCE CODE (Java or SQL)\n- GENERATED WIKI (Markdown)\n\n--- SOURCE CODE ---\n{{source_code}}\n\n--- GENERATED WIKI ---\n{{wiki_md}}\n\nYour task:\nEvaluate whether the Wiki accurately and usefully documents the source code.\n\nThe source language MAY be:\n- Java (classes, methods, fields)\n- SQL (procedures, tables, columns, parameters)\n\nYou MUST judge based ONLY on the given content.\nYou MUST NOT ask for more files or clarification.\n\nEvaluation dimensions (0â€“25 each):\n\n1. Coverage (0â€“25)\n   - Are all major components documented?\n   - Java: classes, public methods, important fields\n   - SQL: procedures, parameters, tables, key logic\n\n2. Correctness (0â€“25)\n   - Are names, signatures, and behavior described correctly?\n   - No wrong return types, parameters, or purposes\n\n3. Hallucination (0â€“25)\n   - 25 = no invented behavior or objects\n   - Deduct heavily if the Wiki mentions things not present\n\n4. Usefulness (0â€“25)\n   - Is this Wiki useful for a developer maintaining the code?\n   - Clear structure, accurate summaries, meaningful explanations\n\nFinal Score = sum of all four dimensions (0â€“100)\n\nRespond ONLY in valid JSON.\n",
        "provider": "ollama:gpt-oss:120b",
        "metrics": {
          "score": 0,
          "testPassCount": 0,
          "testFailCount": 1,
          "testErrorCount": 0,
          "assertPassCount": 0,
          "assertFailCount": 1,
          "totalLatencyMs": 8004,
          "tokenUsage": {
            "prompt": 3807,
            "completion": 677,
            "cached": 0,
            "total": 4484,
            "numRequests": 1,
            "completionDetails": {
              "reasoning": 0,
              "acceptedPrediction": 0,
              "rejectedPrediction": 0
            },
            "assertions": {
              "total": 569,
              "prompt": 295,
              "completion": 274,
              "cached": 0,
              "numRequests": 0,
              "completionDetails": {
                "reasoning": 0,
                "acceptedPrediction": 0,
                "rejectedPrediction": 0
              }
            }
          },
          "namedScores": {},
          "namedScoresCount": {},
          "cost": 0
        }
      }
    ],
    "results": [
      {
        "cost": 0,
        "error": "The rubric is empty, so there is no statement to evaluate against the output.",
        "gradingResult": {
          "pass": false,
          "score": 0,
          "reason": "The rubric is empty, so there is no statement to evaluate against the output.",
          "namedScores": {},
          "tokensUsed": {
            "total": 569,
            "prompt": 295,
            "completion": 274,
            "cached": 0,
            "numRequests": 0
          },
          "componentResults": [
            {
              "assertion": {
                "type": "llm-rubric",
                "provider": {
                  "id": "ollama:gpt-oss:120b"
                },
                "rubric": "You are an automated evaluation judge.\n\nYou have already received the SOURCE CODE and GENERATED WIKI above.\n\nCRITICAL FORMAT REQUIREMENT:\n\nIf your output does NOT EXACTLY match the required JSON schema:\n- The evaluation is considered INVALID\n- The final result MUST be FAIL\n\nYou MUST:\n- Output ALL fields\n- Use EXACT key names\n- Use EXACT value types\n- Output ONLY a JSON object\n- Do NOT include explanations, markdown, or extra text\nEvaluate and output STRICTLY this JSON:\n\n{\n  \"language\": \"JAVA\" | \"SQL\" | \"UNKNOWN\",\n  \"coverage_score\": 0-25,\n  \"correctness_score\": 0-25,\n  \"hallucination_score\": 0-25,\n  \"usefulness_score\": 0-25,\n  \"total_score\": 0-100,\n  \"missing_items\": [\"<object or concept name>\"],\n  \"hallucinated_items\": [\"<object or concept name>\"],\n  \"summary\": \"<one concise sentence>\",\n  \"result\": \"PASS\" | \"FAIL\"\n}\n\nRules:\n- total_score MUST equal the sum of the four scores\n- language MUST be inferred from the source code\n- result MUST be FAIL if:\n  - hallucination_score < 15\n  - correctness_score < 15\n  - total_score < 70\n- missing_items MUST list important undocumented items\n- hallucinated_items MUST list invented objects (empty if none)\n"
              },
              "pass": false,
              "score": 0,
              "reason": "The rubric is empty, so there is no statement to evaluate against the output.",
              "tokensUsed": {
                "total": 569,
                "prompt": 295,
                "completion": 274,
                "cached": 0,
                "numRequests": 0,
                "completionDetails": {
                  "reasoning": 0,
                  "acceptedPrediction": 0,
                  "rejectedPrediction": 0
                }
              },
              "metadata": {
                "renderedGradingPrompt": "[{\"role\":\"system\",\"content\":\"You are grading output according to a user-specified rubric. If the statement in the rubric is true, then the output passes the test. You respond with a JSON object with this structure: {reason: string, pass: boolean, score: number}\\n\\nExamples:\\n\\n<Output>Hello world</Output>\\n<Rubric>Content contains a greeting</Rubric>\\n{\\\"reason\\\": \\\"the content contains the word 'Hello'\\\", \\\"pass\\\": true, \\\"score\\\": 1.0}\\n\\n<Output>Avast ye swabs, repel the invaders!</Output>\\n<Rubric>Does not speak like a pirate</Rubric>\\n{\\\"reason\\\": \\\"'avast ye' is a common pirate term\\\", \\\"pass\\\": false, \\\"score\\\": 0.0}\"},{\"role\":\"user\",\"content\":\"<Output>\\n{\\\"Coverage\\\":20,\\\"Correctness\\\":23,\\\"Hallucination\\\":25,\\\"Usefulness\\\":22,\\\"TotalScore\\\":90}\\n</Output>\\n<Rubric>\\n\\n</Rubric>\"}]"
              }
            }
          ]
        },
        "id": "38991695-46e8-4fa4-9cfd-19410927bf18",
        "latencyMs": 8004,
        "namedScores": {},
        "prompt": {
          "raw": "You are a senior software engineer reviewing generated documentation.\n\nYou are given:\n- SOURCE CODE (Java or SQL)\n- GENERATED WIKI (Markdown)\n\n--- SOURCE CODE ---\nimport os\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.messages import AIMessage, ToolMessage\nfrom langchain_core.runnables import RunnableConfig\n# NOTE: you must use langchain-core >= 0.3 with Pydantic v2\nfrom pydantic import BaseModel\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom copilotkit import CopilotKitState\nfrom copilotkit.langchain import copilotkit_customize_config\n\n\nclass State(CopilotKitState):\n    # This flag is new\n    ask_human: bool\n\n\nclass RequestAssistance(BaseModel):\n    \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n\n    To use this function, relay the user's 'request' so the expert can provide the right guidance.\n    \"\"\"\n\n    request: str\n\n\ntool = TavilySearchResults(max_results=2)\ntools = [tool]\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n# We can bind the llm to a tool definition, a pydantic model, or a json schema\nllm_with_tools = llm.bind_tools(tools + [RequestAssistance])\n\n\ndef chatbot(state: State, config: RunnableConfig):\n    config = copilotkit_customize_config(config, emit_tool_calls=\"RequestAssistance\")\n    response = llm_with_tools.invoke(state[\"messages\"], config=config)\n    ask_human = False\n    if (\n        response.tool_calls\n        and response.tool_calls[0][\"name\"] == RequestAssistance.__name__\n    ):\n        ask_human = True\n    return {\"messages\": [response], \"ask_human\": ask_human}\n\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_node(\"tools\", ToolNode(tools=[tool]))\n\n\ndef create_response(response: str, ai_message: AIMessage):\n    return ToolMessage(\n        content=response,\n        tool_call_id=ai_message.tool_calls[0][\"id\"],\n    )\n\n\ndef human_node(state: State):\n    new_messages = []\n    if not isinstance(state[\"messages\"][-1], ToolMessage):\n        # Typically, the user will have updated the state during the interrupt.\n        # If they choose not to, we will include a placeholder ToolMessage to\n        # let the LLM continue.\n        new_messages.append(\n            create_response(\"No response from human.\", state[\"messages\"][-1])\n        )\n    return {\n        # Append the new messages\n        \"messages\": new_messages,\n        # Unset the flag\n        \"ask_human\": False,\n    }\n\n\ngraph_builder.add_node(\"human\", human_node)\n\n\ndef select_next_node(state: State):\n    if state[\"ask_human\"]:\n        return \"human\"\n    # Otherwise, we can route as before\n    return tools_condition(state)\n\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    select_next_node,\n    {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"},\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(\"human\", \"chatbot\")\ngraph_builder.set_entry_point(\"chatbot\")\nmemory = MemorySaver()\ngraph = graph_builder.compile(\n    checkpointer=memory,\n    interrupt_before=[\"human\"],\n)\n\n--- GENERATED WIKI ---\n## ğŸ“„ æ–‡ä»¶æ¦‚è§ˆ  \r\n**æ–‡ä»¶è·¯å¾„**ï¼š`D:\\code-wiki\\projects\\agent_120\\langgraph-tutorial-quickstart\\agent-py\\tutorial_quickstart\\agent.py`\r\n\r\n### 1. è¿™æ®µä»£ç åˆ°åº•åœ¨å¹²ä»€ä¹ˆï¼Ÿ  \r\næœ¬æ–‡ä»¶æ¼”ç¤ºäº† **LangGraph** ä¸ **LangChain** çš„ç»„åˆä½¿ç”¨ï¼Œæ„å»ºä¸€ä¸ª **å¯ä¸­æ–­ã€å¯å‡çº§** çš„å¯¹è¯ä»£ç†ï¼ˆAgentï¼‰ï¼š\r\n\r\n| å…³é”®æ¦‚å¿µ | ä½œç”¨ |\r\n|---|---|\r\n| **State** | ç»§æ‰¿è‡ª `CopilotKitState`ï¼Œåœ¨å¯¹è¯çŠ¶æ€ä¸­åŠ å…¥ `ask_human` æ ‡è®°ï¼Œç”¨æ¥æŒ‡ç¤ºæ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥ã€‚ |\r\n| **RequestAssistance** | ç”¨ Pydantic å®šä¹‰çš„å·¥å…·ï¼ˆToolï¼‰ï¼Œå½“ LLM è®¤ä¸ºè‡ªå·±æ— æ³•è‡ªè¡Œå›ç­”æ—¶ï¼Œä¼šè¿”å›æ­¤å·¥å…·è°ƒç”¨ï¼Œè§¦å‘ â€œè¯·æ±‚äººå·¥å¸®åŠ©â€ã€‚ |\r\n| **ToolNode** | åŒ…è£…å¤–éƒ¨å·¥å…·ï¼ˆè¿™é‡Œæ˜¯ `TavilySearchResults`ï¼‰çš„æ‰§è¡ŒèŠ‚ç‚¹ã€‚ |\r\n| **Human Node** | å½“ `ask_human=True` æ—¶ï¼Œæš‚åœ LLMï¼Œç­‰å¾…å¼€å‘è€…ï¼ˆæˆ–å‰ç«¯ï¼‰æ‰‹åŠ¨æä¾› `ToolMessage`ï¼Œå†ç»§ç»­å¯¹è¯ã€‚ |\r\n| **MemorySaver** | åŸºäº LangGraph çš„æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰å®ç°ï¼ŒæŒä¹…åŒ–å¯¹è¯å†å²ï¼Œæ”¯æŒä¸­æ–­æ¢å¤ã€‚ |\r\n| **Graph** | ç”¨ `StateGraph` æŠŠä¸Šè¿°èŠ‚ç‚¹ä¸²è”æˆæœ‰å‘çŠ¶æ€æœºï¼Œå®ç° **LLM â†’ Tool â†’ Human â†’ LLM** çš„å¾ªç¯ã€‚ |\r\n\r\n> **å¯¹æ–°æ¥æ‰‹çš„å¼€å‘è€…**ï¼šè¿™æ®µä»£ç çš„æ ¸å¿ƒä»·å€¼åœ¨äº **â€œå¯ä¸­æ–­çš„ LLM æµç¨‹â€**â€”â€”å½“æ¨¡å‹ä¸ç¡®å®šæˆ–æƒé™å—é™æ—¶ï¼Œè‡ªåŠ¨è½¬å‘äººå·¥ååŠ©ï¼Œè€Œä¸æ˜¯ç›²ç›®è¿”å›é”™è¯¯ç­”æ¡ˆã€‚\r\n\r\n---\r\n\r\n## 2. ä»£ç ç»“æ„ä¸å®ç°ç»†èŠ‚  \r\n\r\n### 2.1 å…³é”®ç±» / å‡½æ•°ä¸€è§ˆ  \r\n\r\n| åç§° | ç±»å‹ | è¯´æ˜ | Wiki é“¾æ¥ |\r\n|---|---|---|---|\r\n| `State` | class | æ‰©å±• `CopilotKitState`ï¼Œæ–°å¢ `ask_human` æ ‡è®° | [State](http://localhost:3000/projects/agent_120/wiki?file_path=D:/code-wiki/projects/agent_120/langgraph-tutorial-quickstart/agent-py/tutorial_quickstart/agent.py) |\r\n| `RequestAssistance` | Pydantic `BaseModel` | ç”¨ä½œ LLM çš„å·¥å…·è°ƒç”¨ï¼Œæºå¸¦ç”¨æˆ·çš„æ±‚åŠ©è¯·æ±‚ | [RequestAssistance](http://localhost:3000/projects/agent_120/wiki?file_path=D:/code-wiki/projects/agent_120/langgraph-tutorial-quickstart/agent-py/tutorial_quickstart/agent.py) |\r\n| `chatbot` | function | LLM ä¸»ä½“é€»è¾‘ï¼šè°ƒç”¨æ¨¡å‹ã€æ£€æµ‹æ˜¯å¦è§¦å‘ `RequestAssistance`ï¼Œå¹¶è¿”å› `ask_human` æ ‡è®° | [chatbot](http://localhost:3000/projects/agent_120/wiki?file_path=D:/code-wiki/projects/agent_120/langgraph-tutorial-quickstart/agent-py/tutorial_quickstart/agent.py) |\r\n| `human_node` | function | å½“ `ask_human=True` æ—¶æ‰§è¡Œï¼šè‹¥å‰ä¸€æ­¥æœªäº§ç”Ÿ `ToolMessage`ï¼Œè¡¥ä¸€ä¸ªå ä½ï¼Œéšåæ¸…é™¤æ ‡è®° | [human_node](http://localhost:3000/projects/agent_120/wiki?file_path=D:/code-wiki/projects/agent_120/langgraph-tutorial-quickstart/agent-py/tutorial_quickstart/agent.py) |\r\n| `select_next_node` | function | æ ¹æ® `ask_human` å†³å®šä¸‹ä¸€ä¸ªæ‰§è¡ŒèŠ‚ç‚¹ï¼ˆHuman æˆ– Toolsï¼‰ | [select_next_node](http://localhost:3000/projects/agent_120/wiki?file_path=D:/code-wiki/projects/agent_120/langgraph-tutorial-quickstart/agent-py/tutorial_quickstart/agent.py) |\r\n| `create_response` | function | å°† LLM çš„ `ToolCall` åŒ…è£…æˆ `ToolMessage`ï¼Œä¾› Human Node ä½¿ç”¨ | [create_response](http://localhost:3000/projects/agent_120/wiki?file_path=D:/code-wiki/projects/agent_120/langgraph-tutorial-quickstart/agent-py/tutorial_quickstart/agent.py) |\r\n\r\n### 2.2 å…³é”®å®ç°æµç¨‹  \r\n\r\n#### 2.2.1 LLM ä¸å·¥å…·ç»‘å®š  \r\n\r\n```python\r\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\",\r\n                    api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\r\nllm_with_tools = llm.bind_tools(tools + [RequestAssistance])\r\n```\r\n\r\n- **ä¸ºä»€ä¹ˆè¦ç»‘å®š**ï¼š`bind_tools` è®©æ¨¡å‹åœ¨ç”Ÿæˆå›å¤æ—¶èƒ½å¤Ÿç›´æ¥è¿”å›ç»“æ„åŒ–çš„ `tool_calls`ï¼Œä»è€Œè§¦å‘åç»­èŠ‚ç‚¹ï¼ˆæœç´¢æˆ–äººå·¥è¯·æ±‚ï¼‰ã€‚  \r\n- **å·¥å…·åˆ—è¡¨**ï¼š`tools` åŒ…å« `TavilySearchResults`ï¼ˆç½‘ç»œæœç´¢ï¼‰ï¼Œ`RequestAssistance` ç”¨äºâ€œå‡çº§â€å¯¹è¯ã€‚\r\n\r\n#### 2.2.2 `chatbot` èŠ‚ç‚¹  \r\n\r\n```python\r\ndef chatbot(state: State, config: RunnableConfig):\r\n    config = copilotkit_customize_config(config, emit_tool_calls=\"RequestAssistance\")\r\n    response = llm_with_tools.invoke(state[\"messages\"], config=config)\r\n    ask_human = False\r\n    if response.tool_calls and response.tool_calls[0][\"name\"] == RequestAssistance.__name__:\r\n        ask_human = True\r\n    return {\"messages\": [response], \"ask_human\": ask_human}\r\n```\r\n\r\n- **æ ¸å¿ƒé€»è¾‘**ï¼š  \r\n  1. **è‡ªå®šä¹‰é…ç½®**ï¼š`emit_tool_calls=\"RequestAssistance\"` å‘Šè¯‰ CopilotKit åªåœ¨éœ€è¦æ—¶è¾“å‡ºè¯¥å·¥å…·è°ƒç”¨ã€‚  \r\n  2. **æ¨¡å‹è°ƒç”¨**ï¼šæŠŠå½“å‰å¯¹è¯å†å²ï¼ˆ`state[\"messages\"]`ï¼‰å–‚ç»™ LLMã€‚  \r\n  3. **æ£€æµ‹å‡çº§è¯·æ±‚**ï¼šè‹¥è¿”å›çš„ `tool_calls` åç§°åŒ¹é… `RequestAssistance`ï¼Œåˆ™æŠŠ `ask_human` è®¾ä¸º `True`ï¼Œè§¦å‘åç»­äººå·¥èŠ‚ç‚¹ã€‚  \r\n\r\n#### 2.2.3 `human_node` èŠ‚ç‚¹  \r\n\r\n```python\r\ndef human_node(state: State):\r\n    new_messages = []\r\n    if not isinstance(state[\"messages\"][-1], ToolMessage):\r\n        new_messages.append(create_response(\"No response from human.\", state[\"messages\"][-1]))\r\n    return {\"messages\": new_messages, \"ask_human\": False}\r\n```\r\n\r\n- **ç›®çš„**ï¼šåœ¨ LLM è¯·æ±‚äººå·¥å¸®åŠ©åï¼Œç­‰å¾…å¤–éƒ¨ç³»ç»Ÿï¼ˆå¦‚å‰ç«¯ï¼‰æ³¨å…¥çœŸå®çš„ `ToolMessage`ã€‚è‹¥æ²¡æœ‰æ³¨å…¥ï¼Œåˆ™è‡ªåŠ¨è¡¥ä¸€ä¸ªå ä½ï¼Œä»¥é˜²æ­¢å›¾æ‰§è¡Œå¡æ­»ã€‚  \r\n- **çŠ¶æ€æ¸…ç†**ï¼šè¿”å›åæŠŠ `ask_human` å¤ä½ï¼Œä¿è¯åç»­å¾ªç¯å›åˆ°æ­£å¸¸è·¯å¾„ã€‚\r\n\r\n#### 2.2.4 èŠ‚ç‚¹è·¯ç”±  \r\n\r\n```python\r\ndef select_next_node(state: State):\r\n    if state[\"ask_human\"]:\r\n        return \"human\"\r\n    return tools_condition(state)\r\n```\r\n\r\n- **å†³ç­–ç‚¹**ï¼š  \r\n  - `ask_human=True` â†’ ç›´æ¥è·³åˆ° `human` èŠ‚ç‚¹ã€‚  \r\n  - å¦åˆ™ä½¿ç”¨ `tools_condition`ï¼ˆLangGraph å†…ç½®ï¼‰åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨æœç´¢å·¥å…·æˆ–ç»“æŸå¯¹è¯ã€‚\r\n\r\n### 2.3 æ•´ä½“æ‰§è¡Œå›¾ï¼ˆMermaidï¼‰\r\n\r\n```mermaid\r\nflowchart TD\r\n    A[\"å…¥å£: chatbot\"] --> B{\"ask_human?\"}\r\n    B -- æ˜¯ --> C[\"human_node\"]\r\n    B -- å¦ --> D{\"tools_condition\"}\r\n    D -- éœ€è¦å·¥å…· --> E[\"tools (ToolNode)\"]\r\n    D -- ä¸éœ€è¦å·¥å…· --> F[\"ç»“æŸ __end__\"]\r\n    E --> A\r\n    C --> A\r\n```\r\n\r\n- **æ‰§è¡Œé¡ºåº**ï¼š  \r\n  1. **chatbot** äº§ç”Ÿå›å¤æˆ– `RequestAssistance` è°ƒç”¨ã€‚  \r\n  2. è‹¥æ ‡è®° `ask_human`ï¼Œè½¬åˆ° **human_node**ï¼ˆç­‰å¾…äººå·¥è¾“å…¥ï¼‰ã€‚  \r\n  3. è‹¥æœªæ ‡è®°ï¼Œåˆ™ä¾æ® `tools_condition` å†³å®šæ˜¯å¦è¿›å…¥ **tools**ï¼ˆç½‘ç»œæœç´¢ï¼‰æˆ–ç›´æ¥ç»“æŸã€‚  \r\n  4. **tools** å®Œæˆåå›åˆ° **chatbot**ï¼Œå½¢æˆé—­ç¯ã€‚\r\n\r\n---\r\n\r\n## 3. ä¾èµ–å…³ç³»ä¸æ¨¡å—äº¤äº’  \r\n\r\n| æ¨¡å— | ä¾èµ–æ–¹å‘ | è¯´æ˜ |\r\n|---|---|---|\r\n| `langchain_anthropic.ChatAnthropic` | â†’ LLM | æä¾› Claude 3.5 Sonnet çš„è°ƒç”¨æ¥å£ã€‚ |\r\n| `langchain_community.tools.tavily_search.TavilySearchResults` | â†’ ToolNode | å®ç°ç½‘ç»œæœç´¢åŠŸèƒ½ï¼Œè¿”å›æœ€å¤š 2 æ¡ç»“æœã€‚ |\r\n| `langgraph.checkpoint.memory.MemorySaver` | â†’ Graph | ä¸ºæ•´ä¸ªçŠ¶æ€æœºæä¾›æŒä¹…åŒ–æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒä¸­æ–­æ¢å¤ã€‚ |\r\n| `copilotkit` ç³»åˆ— (`CopilotKitState`, `copilotkit_customize_config`) | â†’ State / Config | ä¸º LangGraph æ³¨å…¥ CopilotKit çš„è‡ªå®šä¹‰é…ç½®ä¸çŠ¶æ€ç»“æ„ã€‚ |\r\n| `langgraph.prebuilt.ToolNode` | â† tools | å°†å¤–éƒ¨å·¥å…·åŒ…è£…ä¸ºå¯åœ¨å›¾ä¸­è°ƒç”¨çš„èŠ‚ç‚¹ã€‚ |\r\n| `langgraph.graph.StateGraph` | â† æ‰€æœ‰èŠ‚ç‚¹ | è´Ÿè´£æŠŠ `chatbot`ã€`tools`ã€`human` ç»„åˆæˆæœ‰å‘çŠ¶æ€æœºã€‚ |\r\n\r\n> **æ³¨æ„**ï¼š  \r\n> - `RequestAssistance` å¹¶éå¤–éƒ¨åº“å®ç°ï¼Œè€Œæ˜¯æœ¬é¡¹ç›®è‡ªå®šä¹‰çš„ **å·¥å…·æ¨¡å‹**ï¼Œå› æ­¤åœ¨å›¾ä¸­ä¸éœ€è¦å•ç‹¬çš„ `ToolNode`ï¼Œè€Œæ˜¯ç›´æ¥åœ¨ `chatbot` ä¸­æ£€æµ‹å¹¶è®¾ç½® `ask_human`ã€‚  \r\n> - `MemorySaver` é€šè¿‡ `checkpointer=memory` å‚æ•°æ³¨å…¥åˆ° `graph.compile`ï¼Œç¡®ä¿æ¯ä¸€æ­¥çš„çŠ¶æ€éƒ½è¢«æŒä¹…åŒ–ã€‚\r\n\r\n---\r\n\r\n## 4. å¸¸è§ç–‘æƒ‘ & å»ºè®®çš„è°ƒè¯•æ–¹å¼  \r\n\r\n| ç–‘æƒ‘ | è§£é‡Š | è°ƒè¯•å»ºè®® |\r\n|---|---|---|\r\n| **ä¸ºä»€ä¹ˆ `chatbot` é‡Œè¦æ‰‹åŠ¨æ£€æŸ¥ `tool_calls[0][\"name\"]`ï¼Ÿ** | LangChain åªä¼šåœ¨æ¨¡å‹è¿”å›å·¥å…·è°ƒç”¨æ—¶å¡«å…… `tool_calls`ï¼Œä½†æˆ‘ä»¬åªå…³å¿ƒ `RequestAssistance`ï¼ˆå‡çº§è¯·æ±‚ï¼‰ï¼Œæ‰€ä»¥æ˜¾å¼åˆ¤æ–­ã€‚ | åœ¨æœ¬åœ°è¿è¡Œæ—¶æ‰“å° `response.tool_calls`ï¼Œç¡®è®¤æ¨¡å‹æ˜¯å¦çœŸçš„è¿”å›äº†è¯¥å·¥å…·ã€‚ |\r\n| **`human_node` ä¸ºä»€ä¹ˆä¼šè‡ªåŠ¨ç”Ÿæˆ â€œNo response from human.â€ï¼Ÿ** | é˜²æ­¢å›¾åœ¨æ²¡æœ‰å¤–éƒ¨æ³¨å…¥ `ToolMessage` æ—¶å¡æ­»ï¼Œä¿è¯å¯¹è¯å¯ä»¥ç»§ç»­ï¼ˆå³ä½¿æ˜¯å ä½ï¼‰ã€‚ | è‹¥æƒ³å¼ºåˆ¶é˜»å¡ç­‰å¾…çœŸå®äººå·¥è¾“å…¥ï¼Œå¯åˆ é™¤å ä½é€»è¾‘æˆ–åœ¨å‰ç«¯å®ç°å›è°ƒã€‚ |\r\n| **`tools_condition` çš„å†…éƒ¨å®ç°æ˜¯ä»€ä¹ˆï¼Ÿ** | LangGraph æä¾›çš„é»˜è®¤è·¯ç”±ï¼šå¦‚æœæ¶ˆæ¯ä¸­åŒ…å«å·¥å…·è°ƒç”¨åˆ™è½¬åˆ° `tools`ï¼Œå¦åˆ™ç»“æŸã€‚ | ä½¿ç”¨ `print(state)` æŸ¥çœ‹ `tools_condition` çš„åˆ¤æ–­ä¾æ®ã€‚ |\r\n| **å¦‚ä½•æŒä¹…åŒ–å¯¹è¯å†å²åˆ°å¤–éƒ¨æ•°æ®åº“ï¼Ÿ** | æ›¿æ¢ `MemorySaver` ä¸ºè‡ªå®šä¹‰ `Checkpointer` å®ç°ï¼ˆå®ç° `save_checkpoint` / `load_checkpoint`ï¼‰ã€‚ | å‚è€ƒ LangGraph æ–‡æ¡£çš„è‡ªå®šä¹‰æ£€æŸ¥ç‚¹ç¤ºä¾‹ã€‚ |\r\n\r\n---\r\n\r\n## 5. æ‰©å±•æ€è·¯  \r\n\r\n1. **å¢åŠ æ›´å¤šå·¥å…·**ï¼šåœ¨ `tools` åˆ—è¡¨ä¸­åŠ å…¥å¦‚ `WikipediaLookup`, `Calculator` ç­‰ï¼Œåªéœ€åœ¨ `llm_with_tools` é‡Œç»‘å®šå³å¯ã€‚  \r\n2. **ç»†åŒ–å‡çº§ç­–ç•¥**ï¼šå¯ä»¥åœ¨ `RequestAssistance` ä¸­åŠ å…¥ `urgency`ã€`category` ç­‰å­—æ®µï¼Œè®©å‰ç«¯æ ¹æ®ä¸åŒéœ€æ±‚åˆ†é…ä¸åŒä¸“å®¶ã€‚  \r\n3. **å¤šè½®äººå·¥äº¤äº’**ï¼šåœ¨ `human_node` ä¸­ä¿å­˜äººå·¥è¿”å›çš„ `ToolMessage`ï¼Œå¹¶åœ¨åç»­å¾ªç¯ä¸­ç»§ç»­ä½¿ç”¨ï¼Œè€Œä¸æ˜¯ä»…ä½œå ä½ã€‚  \r\n4. **æ—¥å¿—ä¸ç›‘æ§**ï¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹å…¥å£/å‡ºå£åŠ å…¥ç»“æ„åŒ–æ—¥å¿—ï¼ˆå¦‚ `logging.info({\"node\": \"chatbot\", \"state\": state})`ï¼‰ï¼Œä¾¿äºè¿½è¸ªå¼‚å¸¸è·¯å¾„ã€‚  \r\n\r\n---  \r\n\r\n**ç»“è¯­**ï¼šæœ¬æ–‡ä»¶å±•ç¤ºäº†ä¸€ä¸ª **â€œLLM + Tool + Humanâ€** çš„å®Œæ•´é—­ç¯å®ç°ï¼Œæ˜¯æ„å»ºå¯å®¡è®¡ã€å¯å‡çº§ AI åŠ©æ‰‹çš„åŸºç¡€æ¨¡æ¿ã€‚æŒæ¡å…¶ä¸­çš„çŠ¶æ€æ ‡è®°ã€èŠ‚ç‚¹è·¯ç”±ä»¥åŠæ£€æŸ¥ç‚¹æœºåˆ¶åï¼Œä½ å¯ä»¥è½»æ¾åœ¨æ­¤åŸºç¡€ä¸ŠåŠ å…¥ä¸šåŠ¡ä¸“å±å·¥å…·ã€å¤æ‚çš„å¯¹è¯ç­–ç•¥ä»¥åŠæŒä¹…åŒ–å­˜å‚¨ã€‚ç¥å¼€å‘é¡ºåˆ©ï¼\n\nYour task:\nEvaluate whether the Wiki accurately and usefully documents the source code.\n\nThe source language MAY be:\n- Java (classes, methods, fields)\n- SQL (procedures, tables, columns, parameters)\n\nYou MUST judge based ONLY on the given content.\nYou MUST NOT ask for more files or clarification.\n\nEvaluation dimensions (0â€“25 each):\n\n1. Coverage (0â€“25)\n   - Are all major components documented?\n   - Java: classes, public methods, important fields\n   - SQL: procedures, parameters, tables, key logic\n\n2. Correctness (0â€“25)\n   - Are names, signatures, and behavior described correctly?\n   - No wrong return types, parameters, or purposes\n\n3. Hallucination (0â€“25)\n   - 25 = no invented behavior or objects\n   - Deduct heavily if the Wiki mentions things not present\n\n4. Usefulness (0â€“25)\n   - Is this Wiki useful for a developer maintaining the code?\n   - Clear structure, accurate summaries, meaningful explanations\n\nFinal Score = sum of all four dimensions (0â€“100)\n\nRespond ONLY in valid JSON.\n",
          "label": "You are a senior software engineer reviewing generated documentation.\n\nYou are given:\n- SOURCE CODE (Java or SQL)\n- GENERATED WIKI (Markdown)\n\n--- SOURCE CODE ---\n{{source_code}}\n\n--- GENERATED WIKI ---\n{{wiki_md}}\n\nYour task:\nEvaluate whether the Wiki accurately and usefully documents the source code.\n\nThe source language MAY be:\n- Java (classes, methods, fields)\n- SQL (procedures, tables, columns, parameters)\n\nYou MUST judge based ONLY on the given content.\nYou MUST NOT ask for more files or clarification.\n\nEvaluation dimensions (0â€“25 each):\n\n1. Coverage (0â€“25)\n   - Are all major components documented?\n   - Java: classes, public methods, important fields\n   - SQL: procedures, parameters, tables, key logic\n\n2. Correctness (0â€“25)\n   - Are names, signatures, and behavior described correctly?\n   - No wrong return types, parameters, or purposes\n\n3. Hallucination (0â€“25)\n   - 25 = no invented behavior or objects\n   - Deduct heavily if the Wiki mentions things not present\n\n4. Usefulness (0â€“25)\n   - Is this Wiki useful for a developer maintaining the code?\n   - Clear structure, accurate summaries, meaningful explanations\n\nFinal Score = sum of all four dimensions (0â€“100)\n\nRespond ONLY in valid JSON.\n",
          "config": {}
        },
        "promptId": "1f559caa9a1e0372ccf383637f46f55d94cdaf7ca4cc9a896a82b47754e15d44",
        "promptIdx": 0,
        "provider": {
          "id": "ollama:gpt-oss:120b",
          "label": ""
        },
        "response": {
          "output": "{\n  \"Coverage\": 20,\n  \"Correctness\": 23,\n  \"Hallucination\": 25,\n  \"Usefulness\": 22,\n  \"TotalScore\": 90\n}",
          "tokenUsage": {
            "prompt": 3807,
            "completion": 677,
            "total": 4484
          }
        },
        "score": 0,
        "success": false,
        "testCase": {
          "name": "Wiki Alignment Evaluation",
          "vars": {
            "source_code": "file://data\\agent.py.txt",
            "wiki_md": "file://data\\agent.py.md"
          },
          "assert": [
            {
              "type": "llm-rubric",
              "provider": {
                "id": "ollama:gpt-oss:120b"
              },
              "rubric": "You are an automated evaluation judge.\n\nYou have already received the SOURCE CODE and GENERATED WIKI above.\n\nCRITICAL FORMAT REQUIREMENT:\n\nIf your output does NOT EXACTLY match the required JSON schema:\n- The evaluation is considered INVALID\n- The final result MUST be FAIL\n\nYou MUST:\n- Output ALL fields\n- Use EXACT key names\n- Use EXACT value types\n- Output ONLY a JSON object\n- Do NOT include explanations, markdown, or extra text\nEvaluate and output STRICTLY this JSON:\n\n{\n  \"language\": \"JAVA\" | \"SQL\" | \"UNKNOWN\",\n  \"coverage_score\": 0-25,\n  \"correctness_score\": 0-25,\n  \"hallucination_score\": 0-25,\n  \"usefulness_score\": 0-25,\n  \"total_score\": 0-100,\n  \"missing_items\": [\"<object or concept name>\"],\n  \"hallucinated_items\": [\"<object or concept name>\"],\n  \"summary\": \"<one concise sentence>\",\n  \"result\": \"PASS\" | \"FAIL\"\n}\n\nRules:\n- total_score MUST equal the sum of the four scores\n- language MUST be inferred from the source code\n- result MUST be FAIL if:\n  - hallucination_score < 15\n  - correctness_score < 15\n  - total_score < 70\n- missing_items MUST list important undocumented items\n- hallucinated_items MUST list invented objects (empty if none)\n"
            }
          ],
          "options": {},
          "metadata": {}
        },
        "testIdx": 0,
        "vars": {
          "source_code": "file://data\\agent.py.txt",
          "wiki_md": "file://data\\agent.py.md"
        },
        "metadata": {
          "_promptfooFileMetadata": {}
        },
        "failureReason": 1
      }
    ],
    "stats": {
      "successes": 0,
      "failures": 1,
      "errors": 0,
      "tokenUsage": {
        "prompt": 3807,
        "completion": 677,
        "cached": 0,
        "total": 4484,
        "numRequests": 1,
        "completionDetails": {
          "reasoning": 0,
          "acceptedPrediction": 0,
          "rejectedPrediction": 0
        },
        "assertions": {
          "total": 569,
          "prompt": 295,
          "completion": 274,
          "cached": 0,
          "numRequests": 0,
          "completionDetails": {
            "reasoning": 0,
            "acceptedPrediction": 0,
            "rejectedPrediction": 0
          }
        }
      },
      "durationMs": 13888
    }
  },
  "config": {
    "prompts": [
      "You are a senior software engineer reviewing generated documentation.\n\nYou are given:\n- SOURCE CODE (Java or SQL)\n- GENERATED WIKI (Markdown)\n\n--- SOURCE CODE ---\n{{source_code}}\n\n--- GENERATED WIKI ---\n{{wiki_md}}\n\nYour task:\nEvaluate whether the Wiki accurately and usefully documents the source code.\n\nThe source language MAY be:\n- Java (classes, methods, fields)\n- SQL (procedures, tables, columns, parameters)\n\nYou MUST judge based ONLY on the given content.\nYou MUST NOT ask for more files or clarification.\n\nEvaluation dimensions (0â€“25 each):\n\n1. Coverage (0â€“25)\n   - Are all major components documented?\n   - Java: classes, public methods, important fields\n   - SQL: procedures, parameters, tables, key logic\n\n2. Correctness (0â€“25)\n   - Are names, signatures, and behavior described correctly?\n   - No wrong return types, parameters, or purposes\n\n3. Hallucination (0â€“25)\n   - 25 = no invented behavior or objects\n   - Deduct heavily if the Wiki mentions things not present\n\n4. Usefulness (0â€“25)\n   - Is this Wiki useful for a developer maintaining the code?\n   - Clear structure, accurate summaries, meaningful explanations\n\nFinal Score = sum of all four dimensions (0â€“100)\n\nRespond ONLY in valid JSON.\n"
    ],
    "providers": [
      {
        "id": "ollama:gpt-oss:120b"
      }
    ],
    "tests": [
      {
        "name": "Wiki Alignment Evaluation",
        "vars": {
          "source_code": "file://data//agent.py.txt",
          "wiki_md": "file://data//agent.py.md"
        },
        "assert": [
          {
            "type": "llm-rubric",
            "provider": {
              "id": "ollama:gpt-oss:120b"
            },
            "rubric": "You are an automated evaluation judge.\n\nYou have already received the SOURCE CODE and GENERATED WIKI above.\n\nCRITICAL FORMAT REQUIREMENT:\n\nIf your output does NOT EXACTLY match the required JSON schema:\n- The evaluation is considered INVALID\n- The final result MUST be FAIL\n\nYou MUST:\n- Output ALL fields\n- Use EXACT key names\n- Use EXACT value types\n- Output ONLY a JSON object\n- Do NOT include explanations, markdown, or extra text\nEvaluate and output STRICTLY this JSON:\n\n{\n  \"language\": \"JAVA\" | \"SQL\" | \"UNKNOWN\",\n  \"coverage_score\": 0-25,\n  \"correctness_score\": 0-25,\n  \"hallucination_score\": 0-25,\n  \"usefulness_score\": 0-25,\n  \"total_score\": 0-100,\n  \"missing_items\": [\"<object or concept name>\"],\n  \"hallucinated_items\": [\"<object or concept name>\"],\n  \"summary\": \"<one concise sentence>\",\n  \"result\": \"PASS\" | \"FAIL\"\n}\n\nRules:\n- total_score MUST equal the sum of the four scores\n- language MUST be inferred from the source code\n- result MUST be FAIL if:\n  - hallucination_score < 15\n  - correctness_score < 15\n  - total_score < 70\n- missing_items MUST list important undocumented items\n- hallucinated_items MUST list invented objects (empty if none)\n"
          }
        ]
      }
    ],
    "outputPath": [
      "results.json"
    ],
    "extensions": []
  },
  "shareableUrl": null,
  "metadata": {
    "promptfooVersion": "0.120.20",
    "nodeVersion": "v22.2.0",
    "platform": "win32",
    "arch": "x64",
    "exportedAt": "2026-01-30T05:47:22.458Z",
    "evaluationCreatedAt": "2026-01-30T05:47:08.454Z"
  }
}