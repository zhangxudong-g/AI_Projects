# Engineering Fact Judge v3 – Design and Rationale

## System Overview

Engineering Fact Judge v3 is a specialized evaluation system designed to strictly define and enforce the factual boundaries of engineering explanations generated by Large Language Models (LLMs). It serves as a critical gatekeeper in the evaluation pipeline, ensuring that generated documentation remains grounded in the actual source code rather than plausible-sounding but unsupported claims.

### Role in the Full Evaluation Flow

Within the broader evaluation pipeline, Fact Judge v3 acts as a validation layer between:
- **Source Code**: The ground truth reference
- **Generated Wiki/Documentation**: The LLM-produced explanations
- **Downstream Scoring Stages**: Higher-level assessments of utility and engineering value

Fact Judge v3 evaluates whether the generated explanations accurately reflect the source code without introducing fabricated elements, inflated responsibilities, or semantic overreaches.

### Relationship to Other Components

Unlike coverage checking systems that focus on completeness, or style evaluators that assess readability, Fact Judge v3 specifically targets factual accuracy. It operates on the principle that engineering documentation must be anchored in code reality, regardless of how reasonable or useful the explanations might sound.

### Why Fact Judge v3 Exists Separately

Fact Judge v3 exists as a distinct concept because factual accuracy is orthogonal to other evaluation dimensions. A document can be comprehensive and well-written but still contain fabricated elements that could mislead engineers. Conversely, a sparse but accurate document provides a solid foundation for engineering decisions.

## Core Evaluation Principles

### Source Code Supremacy

The source code is the only acceptable ground truth for validating engineering explanations. Industry conventions, common architectural patterns, or terminology that isn't explicitly supported by the code cannot serve as evidence for claims made in the documentation.

This principle ensures that the evaluation system doesn't reward explanations that sound plausible but lack code support, preventing the propagation of architectural assumptions that don't match the actual implementation.

### Explanation ≠ Guessing

Even reasonable assumptions must be explicitly supported by the source code. Common architectural patterns, standard frameworks, or industry best practices are not sufficient evidence for claims made in the documentation. The system distinguishes between "this is what the code does" versus "this is what similar systems typically do."

This prevents the introduction of reasonable-sounding but unsupported explanations that could mislead engineers about the actual system behavior.

### Fabrication Is the Primary Risk

Invented layers, responsibilities, behaviors, or architectural elements are treated as high-risk failures. The system prioritizes identifying and rejecting fabricated content over rewarding comprehensive or well-structured explanations that contain inaccuracies.

This approach reflects the engineering reality that incorrect information is often more harmful than incomplete information, especially when making decisions about system modifications or integrations.

### Engineering Usefulness Is Secondary to Truth

While helpful explanations are valuable, correctness takes precedence over utility. A helpful but incorrect explanation is considered worse than an incomplete but accurate one. This trade-off ensures that engineers can trust the documentation as a reliable reference for understanding system behavior.

## Fact Violation Categories

### Architectural Fabrication

**Definition**: The introduction of architectural elements that don't exist in the source code, such as invented services, controllers, frameworks, or transaction management systems.

**Triggering Statements**: Claims about architectural components, layers, or infrastructure that aren't present in the code. For example, describing a caching layer that doesn't exist, or claiming the presence of a microservices architecture when the code is monolithic.

**Danger to Engineering Work**: Architectural fabrications can lead engineers to make incorrect assumptions about system dependencies, scalability characteristics, or integration points, potentially causing significant rework or system failures.

### Responsibility Inflation

**Definition**: Assigning responsibilities, duties, or capabilities to code elements that aren't supported by the actual implementation.

**Triggering Statements**: Describing functions, classes, or modules as performing tasks they don't actually perform, or claiming broader capabilities than what the code implements. For example, stating that a simple data access function handles complex business logic.

**Danger to Engineering Work**: Responsibility inflation can lead to incorrect assumptions about system behavior, causing engineers to rely on capabilities that don't exist or to overlook actual implementation details.

### Mechanism Mismatch

**Definition**: Describing system behavior that differs from what the code actually implements, even if the described behavior seems reasonable.

**Triggering Statements**: Explaining how the system works in ways that contradict the actual code implementation. For example, describing synchronous processing when the code implements asynchronous behavior.

**Danger to Engineering Work**: Mechanism mismatches can lead to incorrect expectations about system performance, error handling, or timing, potentially causing integration issues or debugging difficulties.

### Terminology Hallucination

**Definition**: Treating design patterns, architectural concepts, or industry terms as facts without explicit evidence in the code.

**Triggering Statements**: Claiming the use of specific design patterns, architectural styles, or industry standards without code support. For example, describing a system as using the "Observer Pattern" when the code doesn't implement that pattern.

**Danger to Engineering Work**: Terminology hallucinations can mislead engineers about the system's actual architecture and design decisions, affecting their understanding of how to extend or modify the system.

### Semantic Overreach (Especially for SQL)

**Definition**: Inferring concurrency, isolation, business semantics, or other advanced behaviors that aren't explicitly shown in the code.

**Triggering Statements**: Assuming transactional behavior, isolation levels, or business logic implications that aren't evident in the SQL code. For example, claiming that queries are thread-safe when the code doesn't implement synchronization.

**Danger to Engineering Work**: Semantic overreach can lead to incorrect assumptions about data integrity, concurrency handling, or business rule enforcement, potentially causing data corruption or logical errors.

## Regression-Driven Design

Engineering Fact Judge v3 is protected by a comprehensive suite of regression cases that define non-negotiable boundaries for acceptable behavior. These regression cases serve as authoritative specifications for the system's expected behavior.

### Authority of Regression Cases

Regression cases represent concrete examples of acceptable and unacceptable explanation patterns. They provide objective criteria that take precedence over subjective model judgments or human preferences. Any future changes to the evaluation system must preserve the outcomes of these regression cases.

### Importance of Passing Regression

Passing regression tests is more important than achieving high scores on subjective quality measures. The regression cases represent the minimum viable behavior for the system, ensuring that critical failure modes remain detected and prevented.

### Non-Negotiable Boundaries

The regression cases establish "non-negotiable boundaries" that define what constitutes acceptable engineering documentation. These boundaries protect against the gradual degradation of evaluation standards that could allow fabricated content to slip through.

## What Fact Judge v3 Does Not Do

### Judging Writing Style or Fluency

The system does not evaluate prose quality, readability, or communication effectiveness. These aspects are handled by other evaluation stages that can operate on the foundation of factually accurate content.

### Rewarding Verbosity

Longer explanations are not inherently better than shorter ones. The system focuses on factual accuracy rather than comprehensiveness, allowing downstream systems to evaluate completeness separately.

### Inferring Intent Beyond the Code

The system does not attempt to understand developer intentions, business requirements, or design goals that aren't explicitly reflected in the code. It evaluates only what the code actually implements, not what it might be intended to achieve.

### Assuming Standard Frameworks Unless Shown

The system does not assume the presence of standard frameworks, libraries, or architectural patterns unless they are explicitly evident in the code. This prevents the attribution of framework-specific behaviors to custom implementations.

## Design Consequences

### Low Scores for Many LLM-Generated Documents

Many LLM-generated documents receive low scores because they naturally tend toward reasonable-sounding but unsupported explanations. This is an intentional consequence of prioritizing factual accuracy over plausibility.

### Preference for Conservative Explanations

The system rewards conservative explanations that stick closely to the code, even if they seem incomplete. This creates a bias toward accuracy that supports reliable engineering decisions.

### Strict Feeling Compared to Human Reviewers

The system may feel stricter than human reviewers who might accept reasonable-sounding explanations. This strictness is an intentional engineering trade-off that prioritizes reliability over leniency.

### Intentional Engineering Trade-off

The design represents a conscious decision to prioritize factual accuracy over other desirable qualities like helpfulness or comprehensiveness. This trade-off reflects the engineering principle that incorrect information is more harmful than incomplete information in technical contexts.

## Conclusion

Engineering Fact Judge v3 serves as a critical quality gate in the evaluation pipeline, ensuring that generated engineering documentation remains grounded in source code reality. Through its focus on factual accuracy, comprehensive violation categories, and regression-driven design, it provides a reliable foundation for downstream evaluation stages while protecting against the propagation of fabricated or incorrect information that could mislead engineers.